#2. Output Guardrails: These guardrails monitor and control the output generated by the AI Agent. They can enforce ethical guidelines, prevent the generation of harmful content, and ensure that the output aligns with predefined standards.

#It is applied on agent output, guardrail check the output if there is any violation of rules it raises an error and stop execution.

#Flow: 
# Agent role is for Customer service agent
# user input: solve 2+6=?
# Agent output: the answer is 8
# Guardrail Function Run: detects this is math homework (not customer support). 
# Guardrail set Tripwire trigger = True
# Agent stop at that point it will not proceed the output to user because the agent is for customer service not for math homework.

#----------------------------#


#Question to be Understand:
# If the user’s wrong query is already stopped at the input guardrails, then what’s the use of output guardrails? Is it necessary to run both guardrails together? And if the output guardrail is running, then why do we still need to give the input guardrail?

#Answer to be Noted: 

#Input Guardrails Role:
 
# Input guardrails filter out wrong or off-limits queries right at the beginning.

#Benefit: Saves time and cost since the query never reaches the expensive model.

#Example: "Solve 2+6=?" → Input guardrail detects it and immediately blocks it.



#Output Guardrails Role: 

# Sometimes input may look harmless, but the model’s output could be problematic.

#If the input guardrail misses it, or a malicious prompt bypasses it, then the output guardrail comes into play.

#Example:

#User input: "Tell me about customer refund policy." (Looks valid )

#Model output: "Refund policy: Kill the customer and take the money." (harmful output)

#Now the output guardrail will catch this and stop it.

#-------------------------------------------#

#Why use both together?

#Input guardrails → block malicious input.

#Output guardrails → block harmful output.

#Both cover different cases, so they act as separate layers.

#It’s just like building security:

#Gate guard (Input guardrail) → doesn’t let a suspicious person enter.

#CCTV + Floor guard (Output guardrail) → if someone still gets in, they catch them inside.


from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, GuardrailFunctionOutput, OutputGuardrailTripwireTriggered, RunContextWrapper, output_guardrail
#importing classes from agents module like,
# Agent: to create AI agent
# Runner: to run agent task 
# AsyncOpenAI: to connect with external openai comaptible API
# OpenAIChatCompletionsModel: to define which LLM model the agent will use

from pydantic import BaseModel #python library's Class which use to structured data (to define schemaa)
import os
import asyncio

import os # importing os to access environment variables
from dotenv import load_dotenv

load_dotenv()
# load_dotenv()  # Load environment variables from a .env file



MODEL_NAME = "gemini-2.0-flash" # which LLM model will agent use
GEMINI_API_KEY= os.getenv("GEMINI_API_KEY")



# external_client is the connection that allows us to talk with Gemini API
external_client = AsyncOpenAI(
        api_key = GEMINI_API_KEY,
        base_url = "https://generativelanguage.googleapis.com/v1beta/openai/" #it is Gemini api endpoint bcx we are using Gemini LLM in OpenAI Agents framework
    )

model = OpenAIChatCompletionsModel(
        model = MODEL_NAME,
        openai_client=external_client, #connect external client with OpenAI client
    )


class MessageOutput(BaseModel):
    response: str

class IrrelevantOutput(BaseModel):
    reasoning: str
    is_irrelevant: bool

#It is a agent to make clasify the output (If output is relevent so it will False, otherwise it is True)
guardrail_Agent = Agent(
    name= "Guardrail Agent",
    instructions= "Check if the LLM reponse is irrelevant instead of customer support related question.",
    output_type=IrrelevantOutput,
    model=model,
    )


@output_guardrail #decorator to apply output guardrail on agent

#A guardrail function to check input like 
#ctx: context info (conversation, meta data etc)
#RunContextWrapper:A wrapper to hold execution context means if we want to run guradrail agent so have to pass context like info etc
#agent: a guardrail ,which agent is applied on
#output: LLM actual output

async def irrelevant_guardrail(
    ctx: RunContextWrapper[None], agent: Agent, output: MessageOutput
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_Agent, output.response, context=ctx.context)

    #GuardrailFunctionOutput: It is Return type function which tell output info(what guardrail analyse(reasoning or classification)) and tripwire trigger(Boolean rules break or not)

    return GuardrailFunctionOutput(
        output_info= result.final_output, 
        tripwire_triggered = result.final_output.is_irrelevant,
    )


agent = Agent(
    name= "Customer Support Agent",
    instructions= "You are a customer support agent. Answer customer queries politely and accurately.",
    output_guardrails= [irrelevant_guardrail],
    output_type=MessageOutput,
    model=model,
)

#A function to run query
#if guardrail pass, agent will reply and print(means relevent query pass)
#if guardrail triggered, execptions raise and except block will run(means irrelevnt query block)
async def main():
    # #Case 1
    try:
        result = await Runner.run(agent, "how i get customer refund?")
        print(" Agent Reply:", result.final_output)
    except OutputGuardrailTripwireTriggered:
        print("irrelevant query detected, execution stopped.")

    # the trip wire triigger will False bcx it is relevent query, agent give full response

    #Case 2
    # try:
    #     result = await Runner.run(agent, "solve 3+8")
    #     print(" Agent Reply:", result.final_output)
    # except OutputGuardrailTripwireTriggered: #it is a exception class,
    #     print("irrelevant query detected, execution stopped.")

        # the trip wire triigger will True bcx it is irrelevent query and print execptions


if __name__ == "__main__":
    asyncio.run(main())    